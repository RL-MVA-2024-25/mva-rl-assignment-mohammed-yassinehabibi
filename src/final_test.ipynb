{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import gymnasium as gym\n",
    "from evaluate import evaluate_HIV, evaluate_HIV_population\n",
    "from train import ProjectAgent  # Replace DummyAgent with your agent implementation\n",
    "from statistics import mean\n",
    "from functools import partial\n",
    "from gymnasium.wrappers import TimeLimit\n",
    "from env_hiv import HIVPatient\n",
    "from interface import Agent\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from copy import deepcopy\n",
    "import math\n",
    "import random\n",
    "import matplotlib\n",
    "import time\n",
    "from collections import namedtuple\n",
    "from itertools import count\n",
    "from PIL import Image\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.autograd import Variable\n",
    "from torch.distributions import Categorical\n",
    "import ffmpeg\n",
    "import seaborn as sns\n",
    "import os\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "from dqn_agent import ReplayBuffer, greedy_action, evaluate_agent, dqn_agent\n",
    "from dqn_agent_general import dqn_agent_general, evaluate_agent_general\n",
    "from dqn_agent_2 import dqn_agent_2, evaluate_agent_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "##SAME TEST AS THE ONE OF THE EXAM##\n",
    "def test_as_exam(Agent_class = dqn_agent_2, load_path=\"best_model_standard_scaling_general.pth\", reward_path=\"best_deterministic_reward_2.pth\"):\n",
    "    env = TimeLimit(HIVPatient(domain_randomization=True), max_episode_steps=200)\n",
    "    seed_everything(42)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    # Declare network\n",
    "    state_dim = env.observation_space.shape[0]\n",
    "    n_action = env.action_space.n \n",
    "    nb_neurons=128\n",
    "    config = {'nb_actions': n_action, 'learning_rate': 0.01,\n",
    "            'gamma': 0.95, 'buffer_size': 100_000,\n",
    "            'epsilon_min': 0.01, 'epsilon_max': 1,\n",
    "            'epsilon_decay_period': 5_000, 'epsilon_delay_decay': 600,\n",
    "            'batch_size': 32, 'model_path': 'best_model_standard_scaling_general.pth',\n",
    "            'scaling': 'standard', 'criterion': torch.nn.SmoothL1Loss()}\n",
    "    DQN = torch.nn.Sequential(nn.Linear(state_dim, nb_neurons),\n",
    "                            nn.ReLU(),\n",
    "                            nn.Linear(nb_neurons, nb_neurons),\n",
    "                            nn.ReLU(), \n",
    "                            nn.Linear(nb_neurons, n_action)).to(device)\n",
    "\n",
    "    DQN.load_state_dict(torch.load(load_path, weights_only=True, map_location=device))\n",
    "    print(\"Reward theoretical\", torch.load(reward_path, weights_only=False, map_location=device))\n",
    "    agent = Agent_class(config, DQN, load=True)\n",
    "\n",
    "    score_agent_dr = evaluate_agent(agent, env, nb_episode = 20, scaling='standard')\n",
    "    env = TimeLimit(HIVPatient(), max_episode_steps=200)\n",
    "    score_agent = evaluate_agent(agent, env, nb_episode = 1, scaling='standard')\n",
    "    print(f\"Reward pb1: {score_agent/1_000_000:.2f}M\")\n",
    "    print(f\"Reward pb2: {score_agent_dr/1_000_000:.2f}M\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward theoretical 10204.509430211989\n",
      "Reward pb1: 8932.88M\n",
      "Reward pb2: 11576.35M\n"
     ]
    }
   ],
   "source": [
    "test_as_exam(load_path=\"..\\\\best_model_standard_scaling_general.pth\", reward_path=\"..\\\\best_deterministic_reward_2.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward theoretical 15936.208078343408\n",
      "Reward pb1: 2297.55M\n",
      "Reward pb2: 9051.92M\n"
     ]
    }
   ],
   "source": [
    "test_as_exam(load_path=\"best_model_standard_scaling_general.pth\", reward_path=\"best_deterministic_reward_2.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22410.298104915328\n",
      "Score with domain randomization: 2123.16M\n",
      "Score without domain randomization: 20235.54M\n"
     ]
    }
   ],
   "source": [
    "test_as_exam(Agent_class = dqn_agent, load_path=\"best_model_standard_scaling.pth\", reward_path=\"best_deterministic_reward.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl_hw",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
